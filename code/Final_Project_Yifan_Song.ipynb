{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa44fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from math import floor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import random_split, SubsetRandomSampler\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7ce52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdb(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        strline_L = file.readlines()\n",
    "        # print(strline_L)\n",
    "\n",
    "    X_list = list()\n",
    "    Y_list = list()\n",
    "    Z_list = list()\n",
    "    atomtype_list = list()\n",
    "    for strline in strline_L:\n",
    "        # removes all whitespace at the start and end, including spaces, tabs, newlines and carriage returns\n",
    "        stripped_line = strline.strip()\n",
    "\n",
    "        line_length = len(stripped_line)\n",
    "        # print(\"Line length:{}\".format(line_length))\n",
    "        if line_length < 78:\n",
    "            print(\"ERROR: line length is different. Expected>=78, current={}\".format(line_length))\n",
    "\n",
    "        X_list.append(float(stripped_line[30:38].strip()))\n",
    "        Y_list.append(float(stripped_line[38:46].strip()))\n",
    "        Z_list.append(float(stripped_line[46:54].strip()))\n",
    "\n",
    "        atomtype = stripped_line[76:78].strip()\n",
    "        if atomtype == 'C':\n",
    "            atomtype_list.append('h') # 'h' means hydrophobic\n",
    "        else:\n",
    "            atomtype_list.append('p') # 'p' means polar\n",
    "\n",
    "    return X_list, Y_list, Z_list, atomtype_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eef405f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vox(X_list,Y_list,Z_list,width):\n",
    "        X0 = floor(np.mean(X_list))\n",
    "        Y0 = floor(np.mean(Y_list))\n",
    "        Z0 = floor(np.mean(Z_list))\n",
    "        vox = np.zeros((width*2, width*2, width*2, 4))\n",
    "        x_lower = X0 - width\n",
    "        x_upper = X0 + width\n",
    "        y_lower = Y0 - width\n",
    "        y_upper = Y0 + width\n",
    "        z_lower = Z0 - width\n",
    "        z_upper = Z0 + width\n",
    "        return x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,vox\n",
    "    \n",
    "def convert_xyz_to_vox(x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,X_list, Y_list,Z_list,atomtype_list,dim_1,dim_2,vox):\n",
    "\n",
    "    for j in range(len(X_list)):\n",
    "\n",
    "            x = X_list[j]\n",
    "            y = Y_list[j]\n",
    "            z = Z_list[j]\n",
    "            atomtype = atomtype_list[j]\n",
    "\n",
    "            if x > x_lower and x < x_upper and y > y_lower and y < y_upper and z > z_lower and z < z_upper:\n",
    "                index_x = floor((x-x_lower))\n",
    "                index_y = floor((y-y_lower))\n",
    "                index_z = floor((z-z_lower))\n",
    "                if atomtype =='h':\n",
    "                    vox[index_x,index_y,index_z, dim_1] += 1\n",
    "                elif atomtype =='p':\n",
    "                    vox[index_x, index_y, index_z, dim_2] += 1\n",
    "\n",
    "    return vox\n",
    "\n",
    "def create_pos_or_neg(pos_or_neg):\n",
    "    vox_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for i in range(1,3001): \n",
    "        \n",
    "        lig = i\n",
    "        file_name = \"{}_{}_cg.pdb\".format((\"0000\" + str(lig))[-4:], \"lig\")\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb('training_data/'+file_name)\n",
    "        x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,vox=create_vox(X_list,Y_list,Z_list,20)\n",
    "        vox= convert_xyz_to_vox(x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,X_list, Y_list,Z_list,atomtype_list,0,1,vox)\n",
    "\n",
    "        \n",
    "        if pos_or_neg == 'pos':\n",
    "                pro = i\n",
    "        else:\n",
    "                pro = np.random.choice(np.delete(np.arange(1,3001),i-1))\n",
    "        \n",
    "        file_name = \"{}_{}_cg.pdb\".format((\"0000\" + str(pro))[-4:], \"pro\")\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb('training_data/'+file_name)\n",
    "        vox=convert_xyz_to_vox(x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,X_list, Y_list,Z_list,atomtype_list,2,3,vox)\n",
    "        \n",
    "        vox_list.append(vox)\n",
    "        \n",
    "        if pos_or_neg == 'pos':\n",
    "            label_list.append(1)\n",
    "        else:\n",
    "            label_list.append(0)\n",
    "    return vox_list,label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "58dd8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vox_list, pos_label_list = create_pos_or_neg('pos')\n",
    "neg_vox_list, neg_label_list = create_pos_or_neg('neg')\n",
    "pos_vox_list.extend(neg_vox_list)\n",
    "pos_label_list.extend(neg_label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "21adc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape X\n",
    "X = np.array(pos_vox_list)\n",
    "y = np.array(pos_label_list)\n",
    "X=np.swapaxes(X,1,4)\n",
    "X=np.swapaxes(X,2,4)\n",
    "X=np.swapaxes(X,3,4)\n",
    "X.shape\n",
    "\n",
    "del pos_vox_list\n",
    "del pos_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "99d84f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.2\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer_1=nn.Sequential(\n",
    "            nn.Conv3d(4, 32, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool3d(2,2))\n",
    "        self.layer_2=nn.Sequential(\n",
    "            nn.Conv3d(32, 64, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool3d(2,2))\n",
    "        self.layer_3=nn.Sequential(     \n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool3d(2,2))\n",
    "        self.layer_4=nn.Sequential(   \n",
    "            nn.Conv3d(128, 256, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm3d(256),\n",
    "            nn.ReLU(), \n",
    "            nn.MaxPool3d(2,2))\n",
    "        self.layer_5=nn.Sequential( \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),)\n",
    "        self.layer_6=nn.Sequential( \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 2))\n",
    "    def forward(self, x):\n",
    "        #print(\"in\",x.shape)\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        x = self.layer_4(x)\n",
    "        x = self.layer_5(x)\n",
    "        x = self.layer_6(x)\n",
    "        return x\n",
    "\n",
    "model = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "52e3f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 32, 40, 40, 40]           3,488\n",
      "       BatchNorm3d-2       [-1, 32, 40, 40, 40]              64\n",
      "              ReLU-3       [-1, 32, 40, 40, 40]               0\n",
      "         MaxPool3d-4       [-1, 32, 20, 20, 20]               0\n",
      "            Conv3d-5       [-1, 64, 20, 20, 20]          55,360\n",
      "       BatchNorm3d-6       [-1, 64, 20, 20, 20]             128\n",
      "              ReLU-7       [-1, 64, 20, 20, 20]               0\n",
      "         MaxPool3d-8       [-1, 64, 10, 10, 10]               0\n",
      "            Conv3d-9      [-1, 128, 10, 10, 10]         221,312\n",
      "      BatchNorm3d-10      [-1, 128, 10, 10, 10]             256\n",
      "             ReLU-11      [-1, 128, 10, 10, 10]               0\n",
      "        MaxPool3d-12         [-1, 128, 5, 5, 5]               0\n",
      "           Conv3d-13         [-1, 256, 5, 5, 5]         884,992\n",
      "      BatchNorm3d-14         [-1, 256, 5, 5, 5]             512\n",
      "             ReLU-15         [-1, 256, 5, 5, 5]               0\n",
      "        MaxPool3d-16         [-1, 256, 2, 2, 2]               0\n",
      "          Flatten-17                 [-1, 2048]               0\n",
      "           Linear-18                  [-1, 512]       1,049,088\n",
      "             ReLU-19                  [-1, 512]               0\n",
      "          Dropout-20                  [-1, 512]               0\n",
      "           Linear-21                    [-1, 2]           1,026\n",
      "================================================================\n",
      "Total params: 2,216,226\n",
      "Trainable params: 2,216,226\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.98\n",
      "Forward/backward pass size (MB): 64.86\n",
      "Params size (MB): 8.45\n",
      "Estimated Total Size (MB): 74.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,(4,40,40,40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "949fa127",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_data\n",
    "X=torch.from_numpy(X).float()\n",
    "y=torch.from_numpy(y).long()\n",
    "\n",
    "traindata=[]\n",
    "for i in range(0,len(X)):\n",
    "    traindata.append([X[i],y[i]])\n",
    "\n",
    "# stratified split for validation\n",
    "train_idx, valid_idx= train_test_split(np.arange(len(y)), test_size=0.2, shuffle=True, stratify=y)\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "valid_loader = torch.utils.data.DataLoader(traindata, batch_size=batch_size, sampler=val_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "05df5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay=0\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, \n",
    "                       betas=(0.9, 0.999), eps=1e-08, weight_decay=weight_decay)\n",
    "optimizer_name = 'Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20bdf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "#List to store loss to visualize\n",
    "train_losslist = []\n",
    "train_acclist = []\n",
    "val_losslist = []\n",
    "val_acclist = []\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "#### train the model ####\n",
    "    correct_train= 0\n",
    "    total_train = 0\n",
    "    #model.train()\n",
    "    for i, (inputs,labels) in enumerate(train_loader,0) :\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        #if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    \n",
    "        output = model(inputs)\n",
    "        \n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "    train_acc=correct_train/total_train\n",
    "##### validate the model ####\n",
    "\n",
    "    #model.eval()\n",
    "    correct= 0\n",
    "    total=0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs,labels) in enumerate(valid_loader,0) :\n",
    "            # move tensors to GPU if CUDA is available\n",
    "            #if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(inputs)\n",
    "\n",
    "            # calculate the batch loss\n",
    "            loss = criterion(output, labels)\n",
    "            # update average validation loss \n",
    "            valid_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(output.data,_,predicted,labels)\n",
    "    cur_val_acc = correct / total\n",
    "\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTrain Accuracy:{:.6f} \\tValidation Accuracy:{:.6f}'.format(\n",
    "        epoch, train_loss/len(train_loader), valid_loss/len(valid_loader),train_acc,cur_val_acc))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_project.h5')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "    train_losslist.append(train_loss/len(train_loader))\n",
    "    train_acclist.append(train_acc)\n",
    "    val_losslist.append(valid_loss/len(valid_loader))\n",
    "    val_acclist.append(cur_val_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cac7ce4c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.935,\n",
       " 0.9408333333333333,\n",
       " 0.9408333333333333,\n",
       " 0.9433333333333334,\n",
       " 0.9475,\n",
       " 0.9566666666666667,\n",
       " 0.9641666666666666,\n",
       " 0.9583333333333334,\n",
       " 0.9733333333333334,\n",
       " 0.9675,\n",
       " 0.97,\n",
       " 0.9766666666666667,\n",
       " 0.9791666666666666,\n",
       " 0.9525,\n",
       " 0.9775,\n",
       " 0.9791666666666666,\n",
       " 0.9766666666666667,\n",
       " 0.9691666666666666,\n",
       " 0.9791666666666666,\n",
       " 0.9658333333333333,\n",
       " 0.9725,\n",
       " 0.9566666666666667,\n",
       " 0.9766666666666667,\n",
       " 0.9733333333333334,\n",
       " 0.9783333333333334,\n",
       " 0.9758333333333333,\n",
       " 0.9725,\n",
       " 0.9775,\n",
       " 0.9766666666666667,\n",
       " 0.9758333333333333]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_losslist\n",
    "#train_acclist\n",
    "#val_losslist\n",
    "#val_acclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "# train_loss_diff_batchsize = pd.DataFrame(index = np.arange(len(train_loss)))\n",
    "# val_loss_diff_batchsize = pd.DataFrame(index = np.arange(len(valid_loss)))\n",
    "\n",
    "# #train_loss_diff_batchsize['batch_size_16_train'] = train_loss_batch16\n",
    "# #train_loss_diff_batchsize['batch_size_32_train'] = train_loss_batch32\n",
    "# train_loss_diff_batchsize['batch_size_64_train'] = train_loss_batch64\n",
    "\n",
    "# #train_loss_diff_batchsize['batch_size_16_val'] = val_loss_batch16\n",
    "# #train_loss_diff_batchsize['batch_size_32_val'] = val_loss_batch32\n",
    "# train_loss_diff_batchsize['batch_size_64_val'] = val_loss_batch64\n",
    "\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.set(font_scale=1.5) \n",
    "# sns.lineplot(data = train_loss_diff_batchsize).set(title = 'val vs train Loss', xlabel = 'epoch', ylabel='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cbeb497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data load \n",
    "x_lower = []\n",
    "x_upper = []\n",
    "y_lower = []\n",
    "y_upper = []\n",
    "z_lower = []\n",
    "z_upper = []\n",
    "vox_test_list = []\n",
    "\n",
    "def read_pdb_test(filename):\n",
    "    with open(filename,'r') as file:\n",
    "        strline_L = file.readlines()\n",
    "        # print(strline_L)\n",
    "    X_list = list()\n",
    "    Y_list = list()\n",
    "    Z_list = list()\n",
    "    atomtype_list = list()\n",
    "    for strline in strline_L:\n",
    "\n",
    "        stripped_line = strline.split()\n",
    "\n",
    "        line_length = len(stripped_line)\n",
    "\n",
    "        X_list.append(float(stripped_line[0]))\n",
    "        Y_list.append(float(stripped_line[1]))\n",
    "        Z_list.append(float(stripped_line[2]))\n",
    "\n",
    "        atomtype = stripped_line[3]\n",
    "        if atomtype == 'C':\n",
    "            atomtype_list.append('h') # 'h' means hydrophobic\n",
    "        else:\n",
    "            atomtype_list.append('p') # 'p' means polar\n",
    "\n",
    "    return X_list, Y_list, Z_list, atomtype_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "326767ce",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x00000245BFC8F318>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\newfyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1203, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\newfyp\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1174, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n"
     ]
    }
   ],
   "source": [
    "######test the model######\n",
    "import pandas as pd\n",
    "pred_list = []\n",
    "for i in range(1,825):\n",
    "    vox_list = []\n",
    "    print(i)\n",
    "    file_name = \"{}_{}_cg.pdb\".format((\"0000\" + str(i))[-4:], \"pro\")\n",
    "    pro_X_list, pro_Y_list, pro_Z_list, pro_atomtype_list = read_pdb_test('testing_data_release/testing_data/'+file_name)\n",
    "\n",
    "    #print(len(temp_test_vox_list))\n",
    "    for j in range(1,825):\n",
    "        \n",
    "        lig=j\n",
    "        file_name = \"{}_{}_cg.pdb\".format((\"0000\" + str(lig))[-4:], \"lig\")\n",
    "        X_list, Y_list, Z_list, atomtype_list = read_pdb_test('testing_data_release/testing_data/'+file_name)\n",
    "        x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,vox=create_vox(X_list,Y_list,Z_list,20)\n",
    "        vox= convert_xyz_to_vox(x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,X_list, Y_list,Z_list,atomtype_list,0,1,vox)\n",
    "        vox= convert_xyz_to_vox(x_lower,x_upper,y_lower,y_upper,z_lower,z_upper,pro_X_list, pro_Y_list,pro_Z_list,pro_atomtype_list,2,3,vox)\n",
    "        vox_list.append(vox)\n",
    "        \n",
    "    X_test = np.array(vox_list)\n",
    "    X_test=np.swapaxes(X_test,1,4)\n",
    "    X_test=np.swapaxes(X_test,2,4)\n",
    "    X_test=np.swapaxes(X_test,3,4)              \n",
    "    X_test = torch.from_numpy(X_test).float()\n",
    "    test_loader = torch.utils.data.DataLoader(X_test, batch_size = 1, shuffle=False, num_workers=4)\n",
    "    \n",
    "    model = Net().cuda()\n",
    "    model.load_state_dict(torch.load(\"model_project.h5\"))\n",
    "    model.eval()\n",
    "\n",
    "    prediction = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "            softmax=torch.nn.Softmax()\n",
    "            outputs=softmax(outputs.data.detach().cpu().numpy()[:,1])\n",
    "            prediction.append(outputs)\n",
    "\n",
    "            \n",
    "    pred_index = pd.DataFrame(prediction, index=np.arange(1,len(prediction)+1)).sort_values(by = [0], ascending =False).head(10).index\n",
    "    pred_list.append(pred_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "67101691",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_ouput = pd.DataFrame(pred_list, index=np.arange(1,825), columns=['lig1_id','lig2_id','lig3_id','lig4_id','lig5_id','lig6_id','lig7_id','lig8_id','lig9_id','lig10_id'])\n",
    "Final_ouput.index.names = ['pro_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "61d04401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lig1_id</th>\n",
       "      <th>lig2_id</th>\n",
       "      <th>lig3_id</th>\n",
       "      <th>lig4_id</th>\n",
       "      <th>lig5_id</th>\n",
       "      <th>lig6_id</th>\n",
       "      <th>lig7_id</th>\n",
       "      <th>lig8_id</th>\n",
       "      <th>lig9_id</th>\n",
       "      <th>lig10_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pro_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>388</td>\n",
       "      <td>664</td>\n",
       "      <td>78</td>\n",
       "      <td>156</td>\n",
       "      <td>731</td>\n",
       "      <td>468</td>\n",
       "      <td>213</td>\n",
       "      <td>766</td>\n",
       "      <td>151</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>651</td>\n",
       "      <td>285</td>\n",
       "      <td>85</td>\n",
       "      <td>148</td>\n",
       "      <td>521</td>\n",
       "      <td>367</td>\n",
       "      <td>777</td>\n",
       "      <td>334</td>\n",
       "      <td>315</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593</td>\n",
       "      <td>43</td>\n",
       "      <td>479</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>638</td>\n",
       "      <td>543</td>\n",
       "      <td>60</td>\n",
       "      <td>241</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>701</td>\n",
       "      <td>649</td>\n",
       "      <td>46</td>\n",
       "      <td>666</td>\n",
       "      <td>120</td>\n",
       "      <td>674</td>\n",
       "      <td>421</td>\n",
       "      <td>360</td>\n",
       "      <td>144</td>\n",
       "      <td>227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>90</td>\n",
       "      <td>85</td>\n",
       "      <td>222</td>\n",
       "      <td>91</td>\n",
       "      <td>624</td>\n",
       "      <td>505</td>\n",
       "      <td>204</td>\n",
       "      <td>815</td>\n",
       "      <td>194</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>714</td>\n",
       "      <td>137</td>\n",
       "      <td>407</td>\n",
       "      <td>79</td>\n",
       "      <td>576</td>\n",
       "      <td>606</td>\n",
       "      <td>404</td>\n",
       "      <td>583</td>\n",
       "      <td>202</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>454</td>\n",
       "      <td>426</td>\n",
       "      <td>138</td>\n",
       "      <td>75</td>\n",
       "      <td>368</td>\n",
       "      <td>665</td>\n",
       "      <td>411</td>\n",
       "      <td>335</td>\n",
       "      <td>667</td>\n",
       "      <td>768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>172</td>\n",
       "      <td>589</td>\n",
       "      <td>3</td>\n",
       "      <td>191</td>\n",
       "      <td>544</td>\n",
       "      <td>243</td>\n",
       "      <td>30</td>\n",
       "      <td>690</td>\n",
       "      <td>457</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>790</td>\n",
       "      <td>362</td>\n",
       "      <td>20</td>\n",
       "      <td>565</td>\n",
       "      <td>746</td>\n",
       "      <td>584</td>\n",
       "      <td>450</td>\n",
       "      <td>591</td>\n",
       "      <td>359</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>234</td>\n",
       "      <td>601</td>\n",
       "      <td>350</td>\n",
       "      <td>360</td>\n",
       "      <td>120</td>\n",
       "      <td>166</td>\n",
       "      <td>707</td>\n",
       "      <td>796</td>\n",
       "      <td>211</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        lig1_id  lig2_id  lig3_id  lig4_id  lig5_id  lig6_id  lig7_id  \\\n",
       "pro_id                                                                  \n",
       "1           388      664       78      156      731      468      213   \n",
       "2           651      285       85      148      521      367      777   \n",
       "3           593       43      479       20        3      638      543   \n",
       "4           701      649       46      666      120      674      421   \n",
       "5            90       85      222       91      624      505      204   \n",
       "6           714      137      407       79      576      606      404   \n",
       "7           454      426      138       75      368      665      411   \n",
       "8           172      589        3      191      544      243       30   \n",
       "9           790      362       20      565      746      584      450   \n",
       "10          234      601      350      360      120      166      707   \n",
       "\n",
       "        lig8_id  lig9_id  lig10_id  \n",
       "pro_id                              \n",
       "1           766      151       489  \n",
       "2           334      315       747  \n",
       "3            60      241       777  \n",
       "4           360      144       227  \n",
       "5           815      194       488  \n",
       "6           583      202       661  \n",
       "7           335      667       768  \n",
       "8           690      457       577  \n",
       "9           591      359       170  \n",
       "10          796      211       327  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_ouput.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "642c73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_ouput.to_csv('final_output.txt',sep='\\t',index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c230847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
